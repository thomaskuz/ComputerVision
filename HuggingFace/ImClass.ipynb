{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusmir29/Documents/localDev/Python Projects/ComputerVision/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function huggingface_hub._login.notebook_login(new_session: bool = True, write_permission: bool = False) -> None>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "#notebook login is niet werkend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 30.4kB/s]\n",
      "Downloading data: 100%|██████████| 490M/490M [02:18<00:00, 3.53MB/s] \n",
      "Downloading data: 100%|██████████| 464M/464M [02:13<00:00, 3.49MB/s] \n",
      "Downloading data: 100%|██████████| 472M/472M [02:13<00:00, 3.54MB/s] \n",
      "Downloading data: 100%|██████████| 464M/464M [02:11<00:00, 3.53MB/s] \n",
      "Downloading data: 100%|██████████| 475M/475M [02:14<00:00, 3.52MB/s] \n",
      "Downloading data: 100%|██████████| 470M/470M [02:14<00:00, 3.50MB/s] \n",
      "Downloading data: 100%|██████████| 478M/478M [02:14<00:00, 3.54MB/s] \n",
      "Downloading data: 100%|██████████| 486M/486M [02:23<00:00, 3.38MB/s] \n",
      "Downloading data: 100%|██████████| 423M/423M [02:00<00:00, 3.51MB/s] \n",
      "Downloading data: 100%|██████████| 413M/413M [01:58<00:00, 3.50MB/s] \n",
      "Downloading data: 100%|██████████| 426M/426M [02:00<00:00, 3.53MB/s] \n",
      "Generating train split: 100%|██████████| 75750/75750 [00:02<00:00, 29757.55 examples/s]\n",
      "Generating validation split: 100%|██████████| 25250/25250 [00:00<00:00, 29028.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# duurt mega lang!\n",
    "from datasets import load_dataset\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=512x384>, 'label': 79}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tonen van de image\n",
    "\n",
    "im = food[\"train\"][0][\"image\"]\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# dictionary methods van de dataset class van de HF lib\n",
    "\n",
    "labels = food[\"train\"].features[\"label\"].names\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "# i is de iteratie van de loop, label is de naam uit de list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churros\n"
     ]
    }
   ],
   "source": [
    "# manueel zoeken naar een index \n",
    "print(id2label[str(23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\" #openai/clip-vit-base-patch32\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, use_fast=True) #use fast is beschikbaar bij dit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Apply some image transformations to the images to make the model more robust against overfitting. \n",
    "Here you’ll use torchvision’s transforms module, \n",
    "but you can also use any image library you like.'''\n",
    "\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then create a preprocessing function to apply the transforms and return the pixel_values - the inputs to the model - of the image:\n",
    "\n",
    "\"\"\"\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To apply the preprocessing function over the entire dataset, \n",
    "use 🤗 Datasets with_transform method. \n",
    "The transforms are applied on the fly when you load an element of the dataset:\n",
    "'''\n",
    "\n",
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 12.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Including a metric during training is often helpful for evaluating your model’s performance. \n",
    "You can quickly load an evaluation method with the 🤗 Evaluate library. For this task, \n",
    "load the accuracy metric (see the 🤗 Evaluate quick tour to learn more about how to load and compute a metric):\n",
    "'''\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create a function that passes your predictions and labels to compute to calculate the accuracy:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# trainer is een functie dat werkt met de transformers lib om modellen makkelijker te trainen.\n",
    "# Dit kan via de trainer functie of in pytorch \n",
    "\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "# checkpoint is een AutoImageProcessor\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/186 [00:32<07:52,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.575, 'grad_norm': 1.5091770887374878, 'learning_rate': 2.6315789473684212e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 20/186 [00:58<07:13,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2878, 'grad_norm': 1.9326695203781128, 'learning_rate': 4.970059880239521e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 30/186 [01:24<06:47,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7446, 'grad_norm': 2.172940254211426, 'learning_rate': 4.670658682634731e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 40/186 [01:50<06:20,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3171, 'grad_norm': 2.0049924850463867, 'learning_rate': 4.3712574850299406e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 50/186 [02:16<05:56,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9785, 'grad_norm': 2.2549848556518555, 'learning_rate': 4.07185628742515e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 60/186 [02:43<05:28,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6885, 'grad_norm': 2.1210110187530518, 'learning_rate': 3.7724550898203595e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|███▎      | 62/186 [03:06<05:22,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5255253314971924, 'eval_accuracy': 0.848, 'eval_runtime': 16.7026, 'eval_samples_per_second': 59.871, 'eval_steps_per_second': 3.772, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 70/186 [03:27<05:53,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4691, 'grad_norm': 2.1790575981140137, 'learning_rate': 3.473053892215569e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 80/186 [03:53<04:39,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3159, 'grad_norm': 2.2604119777679443, 'learning_rate': 3.1736526946107784e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 90/186 [04:19<04:09,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1171, 'grad_norm': 2.2836568355560303, 'learning_rate': 2.874251497005988e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 100/186 [04:45<03:42,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0392, 'grad_norm': 2.25778865814209, 'learning_rate': 2.5748502994011976e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 110/186 [05:11<03:17,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.993, 'grad_norm': 2.279264211654663, 'learning_rate': 2.275449101796407e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 120/186 [05:37<02:51,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8562, 'grad_norm': 2.140024423599243, 'learning_rate': 1.9760479041916168e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 125/186 [06:06<02:37,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8053845167160034, 'eval_accuracy': 0.882, 'eval_runtime': 16.0215, 'eval_samples_per_second': 62.416, 'eval_steps_per_second': 3.932, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 130/186 [06:20<03:35,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7909, 'grad_norm': 2.2100229263305664, 'learning_rate': 1.6766467065868263e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 140/186 [06:46<02:01,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7544, 'grad_norm': 3.1372392177581787, 'learning_rate': 1.377245508982036e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 150/186 [07:12<01:33,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.715, 'grad_norm': 2.280329465866089, 'learning_rate': 1.0778443113772455e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 160/186 [07:39<01:07,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6746, 'grad_norm': 2.661790370941162, 'learning_rate': 7.784431137724551e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 170/186 [08:05<00:41,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6164, 'grad_norm': 2.2590699195861816, 'learning_rate': 4.7904191616766475e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 180/186 [08:31<00:15,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6312, 'grad_norm': 2.2427425384521484, 'learning_rate': 1.7964071856287426e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 186/186 [09:04<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6427812576293945, 'eval_accuracy': 0.901, 'eval_runtime': 16.1342, 'eval_samples_per_second': 61.98, 'eval_steps_per_second': 3.905, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [09:05<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 545.5651, 'train_samples_per_second': 21.996, 'train_steps_per_second': 0.341, 'train_loss': 2.4460641901980162, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=186, training_loss=2.4460641901980162, metrics={'train_runtime': 545.5651, 'train_samples_per_second': 21.996, 'train_steps_per_second': 0.341, 'total_flos': 9.232831524962304e+17, 'train_loss': 2.4460641901980162, 'epoch': 2.976})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
    "image = ds[\"image\"][0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'beignets', 'score': 0.970130205154419},\n",
       " {'label': 'ramen', 'score': 0.6129050254821777},\n",
       " {'label': 'prime_rib', 'score': 0.5952610373497009},\n",
       " {'label': 'bruschetta', 'score': 0.5819064378738403},\n",
       " {'label': 'pork_chop', 'score': 0.5796873569488525}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model/checkpoint-186\", device='mps')\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
